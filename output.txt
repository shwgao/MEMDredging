Testing model: climax
===================
Config 1: batch_aggregate=false, checkpointing=false, offload_optimizer=false
torch.cuda.is_available(): True
Running with batch_aggregate=False, checkpointing=False, offload_optimizer=False
Model parameters: 108088512, 412.324951171875 MB
Input memory: 22.359580993652344 MB
Allocated memory: 0.4245 GB
Allocated memory: 0.4245 GB
Mean time/batch:    5.355         seconds
Time  Std:          0.508         seconds
Memory usage:       41.98          GB
Memory std:         0             GB
Throughput:         10.08(8.851~11.74) samples/second

-------------------
Config 2: batch_aggregate=false, checkpointing=false, offload_optimizer=true
torch.cuda.is_available(): True
Running with batch_aggregate=False, checkpointing=False, offload_optimizer=True
Model parameters: 108088512, 412.324951171875 MB
Input memory: 22.359580993652344 MB
Allocated memory: 0.4245 GB
Allocated memory: 0.4245 GB
Mean time/batch:    5.128         seconds
Time  Std:          0.358         seconds
Memory usage:       41.98          GB
Memory std:         0             GB
Throughput:         10.53(9.144~11.99) samples/second

-------------------
Config 3: batch_aggregate=false, checkpointing=true, offload_optimizer=false
torch.cuda.is_available(): True
Running with batch_aggregate=False, checkpointing=True, offload_optimizer=False
Model parameters: 108088512, 412.324951171875 MB
Input memory: 22.359580993652344 MB
Allocated memory: 0.4245 GB
Allocated memory: 0.4245 GB
Mean time/batch:    4.984         seconds
Time  Std:          0.022         seconds
Memory usage:       41.98          GB
Memory std:         0             GB
Throughput:         10.83(10.69~10.86) samples/second

-------------------
Config 4: batch_aggregate=true, checkpointing=true, offload_optimizer=false
torch.cuda.is_available(): True
Running with batch_aggregate=True, checkpointing=True, offload_optimizer=False
Model parameters: 108088512, 412.324951171875 MB
Input memory: 22.359580993652344 MB
Allocated memory: 0.4245 GB
Allocated memory: 0.4245 GB
Traceback (most recent call last):
  File "/nfs/hpc/share/gaosho/projects/mem_proj/MEMDredging/profiling/profiling.py", line 256, in <module>
    single_profile(args, model)
  File "/nfs/hpc/share/gaosho/projects/mem_proj/MEMDredging/profiling/profiling.py", line 79, in single_profile
    return profiler.compute_throughput(data_loader, batch_size=args.batch_size, mode=args.mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/nfs/hpc/share/gaosho/projects/mem_proj/MEMDredging/profiling/profile_tools2.py", line 132, in compute_throughput
    return self.compute_eager_throughput(data_loader, batch_size, warmup, iter)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/nfs/hpc/share/gaosho/projects/mem_proj/MEMDredging/profiling/profile_tools2.py", line 259, in compute_eager_throughput
    time_list, memory_list = self.compute_eager_batched_data(data_loader, start_event, end_event, warmup, iter)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/nfs/hpc/share/gaosho/projects/mem_proj/MEMDredging/profiling/profile_tools2.py", line 189, in compute_eager_batched_data
    self._compute(data_loader, start_event, end_event)
  File "/nfs/hpc/share/gaosho/projects/mem_proj/MEMDredging/profiling/profile_tools2.py", line 143, in _compute
    loss = self.model(data)
           ^^^^^^^^^^^^^^^^
  File "/nfs/hpc/share/gaosho/conda_envs/SAM/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/nfs/hpc/share/gaosho/conda_envs/SAM/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/nfs/hpc/share/gaosho/projects/mem_proj/MEMDredging/src/climax.py", line 384, in forward
    out_transformers = self.forward_encoder(x, lead_times, variables)  # B, L, D
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/nfs/hpc/share/gaosho/projects/mem_proj/MEMDredging/src/climax.py", line 351, in forward_encoder
    x = x + lead_time_emb  # B, L, D
        ~~^~~~~~~~~~~~~~~
RuntimeError: The size of tensor a (48) must match the size of tensor b (54) at non-singleton dimension 0

-------------------
Config 5: batch_aggregate=false, checkpointing=false, offload_optimizer=false, torch_compile=True
torch.cuda.is_available(): True
Running with batch_aggregate=False, checkpointing=False, offload_optimizer=False
Model parameters: 108088512, 412.324951171875 MB
Input memory: 22.359580993652344 MB
Allocated memory: 0.4245 GB
Allocated memory: 0.4245 GB
Mean time/batch:    4.202         seconds
Time  Std:          0.017         seconds
Memory usage:       36.91          GB
Memory std:         0             GB
Throughput:         12.85(12.76~12.96) samples/second

-------------------
Testing model: enformer
===================
Config 1: batch_aggregate=false, checkpointing=false, offload_optimizer=false
torch.cuda.is_available(): True
Running with batch_aggregate=False, checkpointing=False, offload_optimizer=False
Model parameters: 251221292, 958.3331756591797 MB
Input memory: 14.410491943359375 MB
Allocated memory: 0.9523 GB
Allocated memory: 0.9523 GB
Mean time/batch:    3.670         seconds
Time  Std:          0.002         seconds
Memory usage:       41          GB
Memory std:         0             GB
Throughput:         0.8174(0.8168~0.8181) samples/second

-------------------
Config 2: batch_aggregate=false, checkpointing=false, offload_optimizer=true
torch.cuda.is_available(): True
Running with batch_aggregate=False, checkpointing=False, offload_optimizer=True
Model parameters: 251221292, 958.3331756591797 MB
Input memory: 14.410491943359375 MB
Allocated memory: 0.9523 GB
Allocated memory: 0.9523 GB
Mean time/batch:    3.675         seconds
Time  Std:          0.003         seconds
Memory usage:       41          GB
Memory std:         0             GB
Throughput:         0.8163(0.8157~0.8175) samples/second

-------------------
Config 3: batch_aggregate=false, checkpointing=true, offload_optimizer=false
/nfs/hpc/share/gaosho/conda_envs/SAM/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
torch.cuda.is_available(): True
Running with batch_aggregate=False, checkpointing=True, offload_optimizer=False
Model parameters: 251221292, 958.3331756591797 MB
Input memory: 14.410491943359375 MB
Allocated memory: 0.9523 GB
Allocated memory: 0.9523 GB
Mean time/batch:    4.165         seconds
Time  Std:          0.001         seconds
Memory usage:       29.02          GB
Memory std:         0             GB
Throughput:         0.7203(0.7199~0.7207) samples/second

-------------------
Config 4: batch_aggregate=true, checkpointing=true, offload_optimizer=false
/nfs/hpc/share/gaosho/conda_envs/SAM/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
torch.cuda.is_available(): True
Running with batch_aggregate=True, checkpointing=True, offload_optimizer=False
Model parameters: 251221292, 958.3331756591797 MB
Input memory: 14.410491943359375 MB
Allocated memory: 0.9523 GB
Allocated memory: 0.9523 GB
Mean time/batch:    2.055         seconds
Time  Std:          0.002         seconds
Memory usage:       7.11          GB
Memory std:         0             GB
Throughput:         1.46(1.457~1.463) samples/second

-------------------
Config 5: batch_aggregate=false, checkpointing=false, offload_optimizer=false, torch_compile=True
torch.cuda.is_available(): True
Running with batch_aggregate=False, checkpointing=False, offload_optimizer=False
Model parameters: 251221292, 958.3331756591797 MB
Input memory: 14.410491943359375 MB
Allocated memory: 0.9523 GB
Allocated memory: 0.9523 GB
Mean time/batch:    3.230         seconds
Time  Std:          0.004         seconds
Memory usage:       35.54          GB
Memory std:         0             GB
Throughput:         0.9288(0.9276~0.9313) samples/second

-------------------
Testing model: cosmoflow
===================
Config 1: batch_aggregate=false, checkpointing=false, offload_optimizer=false
torch.cuda.is_available(): True
Running with batch_aggregate=False, checkpointing=False, offload_optimizer=False
Model parameters: 1352588, 5.1597137451171875 MB
Input memory: 4992.002380371094 MB
Allocated memory: 4.88 GB
Allocated memory: 4.88 GB
Mean time/batch:    1.586         seconds
Time  Std:          0.001         seconds
Memory usage:       37.82          GB
Memory std:         0             GB
Throughput:         98.33(98.26~98.46) samples/second

-------------------
Config 2: batch_aggregate=false, checkpointing=false, offload_optimizer=true
torch.cuda.is_available(): True
Running with batch_aggregate=False, checkpointing=False, offload_optimizer=True
Model parameters: 1352588, 5.1597137451171875 MB
Input memory: 4992.002380371094 MB
Allocated memory: 4.88 GB
Allocated memory: 4.88 GB
Mean time/batch:    1.591         seconds
Time  Std:          0.004         seconds
Memory usage:       37.82          GB
Memory std:         0             GB
Throughput:         98.05(97.68~98.29) samples/second

-------------------
Config 3: batch_aggregate=false, checkpointing=true, offload_optimizer=false
/nfs/hpc/share/gaosho/conda_envs/SAM/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
torch.cuda.is_available(): True
Running with batch_aggregate=False, checkpointing=True, offload_optimizer=False
Model parameters: 1352588, 5.1597137451171875 MB
Input memory: 4992.002380371094 MB
Allocated memory: 4.88 GB
Allocated memory: 4.88 GB
Mean time/batch:    0.664         seconds
Time  Std:          0.000         seconds
Memory usage:       24.41          GB
Memory std:         0             GB
Throughput:         234.9(234.6~235.1) samples/second

-------------------
Config 4: batch_aggregate=true, checkpointing=true, offload_optimizer=false
/nfs/hpc/share/gaosho/conda_envs/SAM/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
torch.cuda.is_available(): True
Running with batch_aggregate=True, checkpointing=True, offload_optimizer=False
Model parameters: 1352588, 5.1597137451171875 MB
Input memory: 4992.002380371094 MB
Allocated memory: 4.88 GB
Allocated memory: 4.88 GB
Mean time/batch:    0.631         seconds
Time  Std:          0.001         seconds
Memory usage:       6.995          GB
Memory std:         0             GB
Throughput:         247.3(246.9~247.7) samples/second

-------------------
Config 5: batch_aggregate=false, checkpointing=false, offload_optimizer=false, torch_compile=True
torch.cuda.is_available(): True
Running with batch_aggregate=False, checkpointing=False, offload_optimizer=False
Model parameters: 1352588, 5.1597137451171875 MB
Input memory: 4992.002380371094 MB
Allocated memory: 4.88 GB
Allocated memory: 4.88 GB
Mean time/batch:    1.522         seconds
Time  Std:          0.004         seconds
Memory usage:       30.5          GB
Memory std:         0             GB
Throughput:         102.5(102~102.8) samples/second

-------------------
Testing model: sam
===================
Config 1: batch_aggregate=false, checkpointing=false, offload_optimizer=false
torch.cuda.is_available(): True
Running with batch_aggregate=False, checkpointing=False, offload_optimizer=False
Model parameters: 93735472, 357.57244873046875 MB
Input memory: 12.5 MB
Allocated memory: 0.42 GB
Allocated memory: 0.42 GB
Mean time/batch:    2.347         seconds
Time  Std:          0.002         seconds
Memory usage:       41.92          GB
Memory std:         0             GB
Throughput:         2.131(2.128~2.134) samples/second

-------------------
Config 2: batch_aggregate=false, checkpointing=false, offload_optimizer=true
torch.cuda.is_available(): True
Running with batch_aggregate=False, checkpointing=False, offload_optimizer=True
Model parameters: 93735472, 357.57244873046875 MB
Input memory: 12.5 MB
Allocated memory: 0.42 GB
Allocated memory: 0.42 GB
Mean time/batch:    2.346         seconds
Time  Std:          0.003         seconds
Memory usage:       41.92          GB
Memory std:         0             GB
Throughput:         2.132(2.129~2.137) samples/second

-------------------
Config 3: batch_aggregate=false, checkpointing=true, offload_optimizer=false
/nfs/hpc/share/gaosho/conda_envs/SAM/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
torch.cuda.is_available(): True
Running with batch_aggregate=False, checkpointing=True, offload_optimizer=False
Model parameters: 93735472, 357.57244873046875 MB
Input memory: 12.5 MB
Allocated memory: 0.42 GB
Allocated memory: 0.42 GB
Mean time/batch:    2.873         seconds
Time  Std:          0.002         seconds
Memory usage:       24.59          GB
Memory std:         0             GB
Throughput:         1.741(1.738~1.743) samples/second

-------------------
Config 4: batch_aggregate=true, checkpointing=true, offload_optimizer=false
/nfs/hpc/share/gaosho/conda_envs/SAM/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
torch.cuda.is_available(): True
Running with batch_aggregate=True, checkpointing=True, offload_optimizer=False
Model parameters: 93735472, 357.57244873046875 MB
Input memory: 12.5 MB
Allocated memory: 0.42 GB
Allocated memory: 0.42 GB
Mean time/batch:    2.899         seconds
Time  Std:          0.028         seconds
Memory usage:       17.45          GB
Memory std:         0             GB
Throughput:         1.725(1.677~1.734) samples/second

-------------------
Config 5: batch_aggregate=false, checkpointing=false, offload_optimizer=false, torch_compile=True
W0410 15:34:24.405000 3602540 site-packages/torch/_dynamo/variables/tensor.py:776] [0/0] Graph break from `Tensor.item()`, consider setting:
W0410 15:34:24.405000 3602540 site-packages/torch/_dynamo/variables/tensor.py:776] [0/0]     torch._dynamo.config.capture_scalar_outputs = True
W0410 15:34:24.405000 3602540 site-packages/torch/_dynamo/variables/tensor.py:776] [0/0] or:
W0410 15:34:24.405000 3602540 site-packages/torch/_dynamo/variables/tensor.py:776] [0/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1
W0410 15:34:24.405000 3602540 site-packages/torch/_dynamo/variables/tensor.py:776] [0/0] to include these operations in the captured graph.
W0410 15:34:24.405000 3602540 site-packages/torch/_dynamo/variables/tensor.py:776] [0/0] 
W0410 15:34:24.405000 3602540 site-packages/torch/_dynamo/variables/tensor.py:776] [0/0] Graph break: from user code at:
W0410 15:34:24.405000 3602540 site-packages/torch/_dynamo/variables/tensor.py:776] [0/0]   File "/nfs/hpc/share/gaosho/projects/mem_proj/MEMDredging/src/sam.py", line 22, in forward
W0410 15:34:24.405000 3602540 site-packages/torch/_dynamo/variables/tensor.py:776] [0/0]     outputs = self.model(**inputs, multimask_output=False)
W0410 15:34:24.405000 3602540 site-packages/torch/_dynamo/variables/tensor.py:776] [0/0]   File "/nfs/hpc/share/gaosho/projects/mem_proj/MEMDredging/src/sam_modeling.py", line 1469, in forward
W0410 15:34:24.405000 3602540 site-packages/torch/_dynamo/variables/tensor.py:776] [0/0]     low_res_masks, iou_predictions, mask_decoder_attentions = self.mask_decoder(
W0410 15:34:24.405000 3602540 site-packages/torch/_dynamo/variables/tensor.py:776] [0/0]   File "/nfs/hpc/share/gaosho/projects/mem_proj/MEMDredging/src/sam_modeling.py", line 499, in forward
W0410 15:34:24.405000 3602540 site-packages/torch/_dynamo/variables/tensor.py:776] [0/0]     if sparse_prompt_embeddings.sum().item() != 0:
W0410 15:34:24.405000 3602540 site-packages/torch/_dynamo/variables/tensor.py:776] [0/0] 
W0410 15:34:24.405000 3602540 site-packages/torch/_dynamo/variables/tensor.py:776] [0/0] 
W0410 15:36:03.356000 3602540 site-packages/torch/fx/experimental/symbolic_shapes.py:5124] [5/0] failed during evaluate_expr(u0, hint=None, size_oblivious=False, forcing_spec=False
E0410 15:36:03.357000 3602540 site-packages/torch/fx/experimental/recording.py:298] [5/0] failed while running evaluate_expr(*(u0, None), **{'fx_node': False})
W0410 15:36:03.395000 3602540 site-packages/torch/fx/experimental/symbolic_shapes.py:5124] [6/0] failed during evaluate_expr(u0, hint=None, size_oblivious=False, forcing_spec=False
E0410 15:36:03.396000 3602540 site-packages/torch/fx/experimental/recording.py:298] [6/0] failed while running evaluate_expr(*(u0, None), **{'fx_node': False})
W0410 15:36:03.450000 3602540 site-packages/torch/fx/experimental/symbolic_shapes.py:5124] [7/0] failed during evaluate_expr(u0, hint=None, size_oblivious=False, forcing_spec=False
E0410 15:36:03.451000 3602540 site-packages/torch/fx/experimental/recording.py:298] [7/0] failed while running evaluate_expr(*(u0, None), **{'fx_node': False})
W0410 15:36:03.481000 3602540 site-packages/torch/fx/experimental/symbolic_shapes.py:5124] [8/0] failed during evaluate_expr(u0, hint=None, size_oblivious=False, forcing_spec=False
E0410 15:36:03.481000 3602540 site-packages/torch/fx/experimental/recording.py:298] [8/0] failed while running evaluate_expr(*(u0, None), **{'fx_node': False})
W0410 15:36:03.525000 3602540 site-packages/torch/_dynamo/exc.py:284] [8/0_1] Backend compiler failed with a fake tensor exception at 
W0410 15:36:03.525000 3602540 site-packages/torch/_dynamo/exc.py:284] [8/0_1]   File "/nfs/hpc/share/gaosho/conda_envs/SAM/lib/python3.12/site-packages/monai/losses/dice.py", line 177, in forward
W0410 15:36:03.525000 3602540 site-packages/torch/_dynamo/exc.py:284] [8/0_1]     intersection = torch.sum(target * input, dim=reduce_axis)
W0410 15:36:03.525000 3602540 site-packages/torch/_dynamo/exc.py:284] [8/0_1] Adding a graph break.
W0410 15:36:03.552000 3602540 site-packages/torch/_dynamo/exc.py:284] [8/0_2] Backend compiler failed with a fake tensor exception at 
W0410 15:36:03.552000 3602540 site-packages/torch/_dynamo/exc.py:284] [8/0_2]   File "/nfs/hpc/share/gaosho/conda_envs/SAM/lib/python3.12/site-packages/monai/losses/dice.py", line 177, in forward
W0410 15:36:03.552000 3602540 site-packages/torch/_dynamo/exc.py:284] [8/0_2]     intersection = torch.sum(target * input, dim=reduce_axis)
W0410 15:36:03.552000 3602540 site-packages/torch/_dynamo/exc.py:284] [8/0_2] Adding a graph break.
W0410 15:37:53.073000 3602540 site-packages/torch/_dynamo/convert_frame.py:844] [3/8] torch._dynamo hit config.cache_size_limit (8)
W0410 15:37:53.073000 3602540 site-packages/torch/_dynamo/convert_frame.py:844] [3/8]    function: 'torch_dynamo_resume_in_forward_at_499' (/nfs/hpc/share/gaosho/projects/mem_proj/MEMDredging/src/sam_modeling.py:499)
W0410 15:37:53.073000 3602540 site-packages/torch/_dynamo/convert_frame.py:844] [3/8]    last reason: 3/0: L['___stack0'] == 908.497802734375                          
W0410 15:37:53.073000 3602540 site-packages/torch/_dynamo/convert_frame.py:844] [3/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
W0410 15:37:53.073000 3602540 site-packages/torch/_dynamo/convert_frame.py:844] [3/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
torch.cuda.is_available(): True
Running with batch_aggregate=False, checkpointing=False, offload_optimizer=False
Model parameters: 93735472, 357.57244873046875 MB
Input memory: 12.5 MB
Allocated memory: 0.42 GB
Allocated memory: 0.42 GB
Mean time/batch:    5.156         seconds
Time  Std:          3.200         seconds
Memory usage:       39.01          GB
Memory std:         0             GB
Throughput:         0.9698(0.573~2.547) samples/second

-------------------
Testing model: simmim
===================
Config 1: batch_aggregate=false, checkpointing=false, offload_optimizer=false
torch.cuda.is_available(): True
Running with batch_aggregate=False, checkpointing=False, offload_optimizer=False
Model parameters: 49338344, 188.21084594726562 MB
Input memory: 336.0 MB
Allocated memory: 0.5119 GB
Allocated memory: 0.5119 GB
Mean time/batch:    1.754         seconds
Time  Std:          0.004         seconds
Memory usage:       39.59          GB
Memory std:         0             GB
Throughput:         3.991(3.979~4.013) samples/second

-------------------
Config 2: batch_aggregate=false, checkpointing=false, offload_optimizer=true
torch.cuda.is_available(): True
Running with batch_aggregate=False, checkpointing=False, offload_optimizer=True
Model parameters: 49338344, 188.21084594726562 MB
Input memory: 336.0 MB
Allocated memory: 0.5119 GB
Allocated memory: 0.5119 GB
Mean time/batch:    1.760         seconds
Time  Std:          0.001         seconds
Memory usage:       39.59          GB
Memory std:         0             GB
Throughput:         3.978(3.975~3.98) samples/second

-------------------
Config 3: batch_aggregate=false, checkpointing=true, offload_optimizer=false
/nfs/hpc/share/gaosho/conda_envs/SAM/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
torch.cuda.is_available(): True
Running with batch_aggregate=False, checkpointing=True, offload_optimizer=False
Model parameters: 49338344, 188.21084594726562 MB
Input memory: 336.0 MB
Allocated memory: 0.5119 GB
Allocated memory: 0.5119 GB
Mean time/batch:    2.323         seconds
Time  Std:          0.002         seconds
Memory usage:       16.83          GB
Memory std:         0             GB
Throughput:         3.013(3.008~3.019) samples/second

-------------------
Config 4: batch_aggregate=true, checkpointing=true, offload_optimizer=false
/nfs/hpc/share/gaosho/conda_envs/SAM/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
torch.cuda.is_available(): True
Running with batch_aggregate=True, checkpointing=True, offload_optimizer=False
Model parameters: 49338344, 188.21084594726562 MB
Input memory: 336.0 MB
Allocated memory: 0.5119 GB
Allocated memory: 0.5119 GB
Mean time/batch:    2.348         seconds
Time  Std:          0.002         seconds
Memory usage:       12.96          GB
Memory std:         0             GB
Throughput:         2.981(2.977~2.984) samples/second

-------------------
Config 5: batch_aggregate=false, checkpointing=false, offload_optimizer=false, torch_compile=True
torch.cuda.is_available(): True
Running with batch_aggregate=False, checkpointing=False, offload_optimizer=False
Model parameters: 49338344, 188.21084594726562 MB
Input memory: 336.0 MB
Allocated memory: 0.5119 GB
Allocated memory: 0.5119 GB
Mean time/batch:    1.414         seconds
Time  Std:          0.003         seconds
Memory usage:       35.98          GB
Memory std:         0             GB
Throughput:         4.949(4.942~4.978) samples/second

-------------------
Testing completed
